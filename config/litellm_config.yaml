# LiteLLM Proxy Configuration
# Maps model names to OpenAI backend with retry and timeout settings

# Model definitions - maps model names to OpenAI backend
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600  # 10 minutes for long requests

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600

  - model_name: gpt-4.1
    litellm_params:
      model: openai/gpt-4.1
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600

  - model_name: gpt-4.1-mini
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600

  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600

  - model_name: gpt-5-mini
    litellm_params:
      model: openai/gpt-5-mini
      api_key: os.environ/OPENAI_API_KEY
      num_retries: 3
      timeout: 600

# Proxy authentication and database
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true

# OTEL integration disabled to reduce trace noise
# Token usage and cost tracking still work via LiteLLM database
# Your app's spans (reasoning-api) still appear in Phoenix
# litellm_settings:
#   callbacks: ["otel"]
#   success_callback: ["otel"]
#   failure_callback: ["otel"]

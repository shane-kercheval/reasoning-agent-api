# Docker Compose Setup Guide

This document explains how to run the Reasoning Agent API project using Docker Compose, making it easy to deploy all services together.

## üöÄ Quick Start

1. **Copy the environment file**:
   ```bash
   cp .env.dev.example .env
   ```

2. **Configure environment variables**:
   ```bash
   # Edit .env file and set:

   # Real OpenAI API key (used by LiteLLM proxy only)
   OPENAI_API_KEY=your-openai-api-key-here

   # Generate LiteLLM master key
   # python -c "import secrets; print('sk-' + secrets.token_urlsafe(32))"
   LITELLM_MASTER_KEY=sk-...

   # Generate LiteLLM PostgreSQL password
   # python -c "import secrets; print(secrets.token_urlsafe(16))"
   LITELLM_POSTGRES_PASSWORD=...

   # Phoenix PostgreSQL password
   PHOENIX_POSTGRES_PASSWORD=phoenix_dev_password123
   ```

3. **Start all services**:

   ```bash
   # Starts all services with Docker Compose (dev mode with hot reload)
   make docker_up
   ```

4. **Setup LiteLLM virtual keys**:
   ```bash
   # Wait for services to be healthy (~30 seconds), then:
   make litellm_setup

   # Copy generated keys to .env:
   # LITELLM_API_KEY=sk-...
   # LITELLM_EVAL_KEY=sk-...

   # Restart reasoning-api to load new keys
   docker compose restart reasoning-api
   ```

5. **Access the services**:
   - **Web Interface**: http://localhost:8080
   - **API Documentation**: http://localhost:8000/docs
   - **API Health Check**: http://localhost:8000/health
   - **MCP Server**: http://localhost:8001/mcp/
   - **Phoenix UI**: http://localhost:6006
   - **LiteLLM Dashboard**: http://localhost:4000

## üìã Services Overview

The Docker Compose setup includes the following services:

### Application Services

#### 1. Reasoning API (`reasoning-api`)

- **Port**: 8000
- **Description**: Main API service with OpenAI compatibility and intelligent request routing
- **Health Check**: http://localhost:8000/health
- **Documentation**: http://localhost:8000/docs
- **Features**: Three routing paths (passthrough, reasoning, orchestration)

#### 2. Web Client (`web-client`)

- **Port**: 8080
- **Description**: MonsterUI web interface
- **Health Check**: http://localhost:8080/health
- **Interface**: http://localhost:8080

### Infrastructure Services

#### 4. LiteLLM Proxy (`litellm`)

- **Port**: 4000
- **Description**: Unified LLM gateway for all API calls
- **Health Check**: http://localhost:4000/health/readiness
- **Dashboard**: http://localhost:4000
- **Features**: Virtual key management, usage tracking, OTEL integration
- **Database**: postgres-litellm (port 5433)

#### 5. PostgreSQL for LiteLLM (`postgres-litellm`)

- **Port**: 5433 (external), 5432 (internal)
- **Description**: Database for LiteLLM virtual keys and usage data
- **Container**: litellm-postgres
- **Data Volume**: litellm_postgres_data

#### 6. PostgreSQL for Phoenix (`postgres-phoenix`)

- **Port**: 5432
- **Description**: Database for Phoenix trace storage
- **Container**: phoenix-postgres
- **Data Volume**: phoenix_postgres_data

#### 7. Phoenix Arize (`phoenix`)

- **Port**: 6006 (Web UI), 4317 (OTLP gRPC)
- **Description**: LLM observability and tracing platform
- **Web UI**: http://localhost:6006
- **Storage**: PostgreSQL with persistent volume
- **Version**: version-11.7

## üîß Configuration

### Environment Variables

The main configuration is in the `.env` file (unified for all services):

```bash
# =============================================================================
# LITELLM PROXY CONFIGURATION
# =============================================================================
LITELLM_MASTER_KEY=            # Admin key for virtual key generation
LITELLM_POSTGRES_PASSWORD=     # Password for LiteLLM database
LITELLM_API_KEY=              # Virtual key for app (generated by litellm_setup)
LITELLM_EVAL_KEY=             # Virtual key for evals (generated by litellm_setup)

# =============================================================================
# SHARED CONFIGURATION
# =============================================================================
OPENAI_API_KEY=your-openai-api-key-here  # Real key (ONLY used by LiteLLM)
REQUIRE_AUTH=false                        # Enable/disable authentication

# Authentication (optional in dev)
# API_TOKENS=web-client-dev-token,admin-dev-token,mobile-dev-token
# REASONING_API_TOKEN=web-client-dev-token

# Service Configuration
REASONING_API_URL=http://localhost:8000
WEB_CLIENT_PORT=8080

# HTTP Client Configuration
HTTP_CONNECT_TIMEOUT=5.0
HTTP_READ_TIMEOUT=60.0
HTTP_WRITE_TIMEOUT=10.0

# =============================================================================
# PHOENIX CONFIGURATION
# =============================================================================
PHOENIX_POSTGRES_PASSWORD=phoenix_dev_password123
```

### MCP Configuration

The MCP servers are configured in `config/mcp_servers.json`. This file uses Docker service names for internal networking:

```json
{
  "mcpServers": {
    "demo_tools": {
      "url": "http://fake-mcp-server:8001/mcp/",
      "transport": "http"
    }
  }
}
```

### LiteLLM Configuration

The LiteLLM proxy is configured via `config/litellm_config.yaml`:

```yaml
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

litellm_settings:
  callbacks: ["otel"]  # Phoenix OTEL integration
```

**Key Features**:
- Virtual key management for environment-based tracking
- OTEL integration with Phoenix for unified observability
- Connection pooling via shared AsyncOpenAI client
- Unlimited budgets (no rate limiting) for development

**Virtual Key Setup**:
1. Start services: `make docker_up`
2. Generate keys: `make litellm_setup`
3. Copy keys to `.env` (LITELLM_API_KEY, LITELLM_EVAL_KEY)
4. Restart API: `docker compose restart reasoning-api`

**Management Commands**:
```bash
make litellm_ui          # Open LiteLLM dashboard
make litellm_logs        # View LiteLLM logs
make litellm_restart     # Restart LiteLLM service
make litellm_reset       # Reset database (DESTRUCTIVE)
```

## üõ†Ô∏è Development Workflow

### Building and Running

```bash
# Build all services
docker-compose build

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop all services
docker-compose down
```

### Individual Service Management

```bash
# Start only specific services
docker-compose up -d reasoning-api fake-mcp-server

# Rebuild a specific service
docker-compose build reasoning-api

# View logs for specific service
docker-compose logs -f web-client
```

### Development Mode

For development with code changes:

```bash
# Stop the services
docker-compose down

# Rebuild after code changes
docker-compose build

# Start with logs visible
docker-compose up
```

## üîç Testing the Setup

### 1. Health Checks

```bash
# Check API health
curl http://localhost:8000/health

# Check web client health
curl http://localhost:8080/health

# Check MCP server health
curl http://localhost:8001/
```

### 2. API Testing

```bash
# Test the API directly
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer web-client-dev-token" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "What is the weather like?"}],
    "stream": false
  }'
```

### 3. Web Interface Testing

1. Open http://localhost:8080 in your browser
2. Try asking: "What's the weather like in Paris?"
3. The system should use the fake MCP server to provide weather data

## üîß Troubleshooting

### Common Issues

1. **Port conflicts**:
   ```bash
   # Check if ports are in use
   lsof -i :8000 :8080 :8001
   
   # Kill processes if needed
   make cleanup
   ```

2. **Permission issues**:
   ```bash
   # Fix Docker permissions
   sudo chown -R $USER:$USER .
   ```

3. **Service not starting**:
   ```bash
   # Check logs
   docker-compose logs service-name
   
   # Check health
   docker-compose ps
   ```

### Debug Mode

```bash
# Run with verbose logging
docker-compose up --build

# Check specific service logs
docker-compose logs -f reasoning-api
```

## üîÑ Updates and Maintenance

### Updating Dependencies

```bash
# Update base images
docker-compose pull

# Rebuild with latest changes
docker-compose build --no-cache

# Restart services
docker-compose down && docker-compose up -d
```

### Adding New MCP Servers

1. **Create a new Dockerfile** (e.g., `Dockerfile.new-mcp`)
2. **Add service to docker-compose.yml**:
   ```yaml
   new-mcp-server:
     build:
       context: .
       dockerfile: Dockerfile.new-mcp
     ports:
       - "8002:8002"
     networks:
       - reasoning-network
   ```
3. **Update MCP configuration** in `config/mcp_servers.docker.yaml`
4. **Rebuild and restart**

## üìä Monitoring

### Service Status

```bash
# Check all services
docker-compose ps

# Check resource usage
docker stats

# Check logs
docker-compose logs -f --tail=100
```

### Health Monitoring

All services include health checks that can be monitored:

```bash
# Check health status
docker-compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}"
```

## üí° Development vs Docker

### Local Development (Current)
```bash
# Terminal 1: Start API
make api

# Terminal 2: Start MCP server
make demo_mcp_server  

# Terminal 3: Start web client
make web_client
```

### Docker Development
```bash
# Single command starts everything
docker-compose up -d

# View all logs
docker-compose logs -f
```

Both approaches work great - Docker is better for deployment and consistency, while local development is better for rapid iteration and debugging.

This setup provides a robust, scalable foundation for your reasoning agent project that can grow with your needs!

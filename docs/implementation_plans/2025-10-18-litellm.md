LiteLLM Proxy Integration - Implementation Plan

  Overview

  Goal: Replace direct OpenAI API calls with LiteLLM proxy to enable unified observability, cost tracking, and rate limiting across all LLM requests.

  Architecture Change:
  BEFORE: <code> → OpenAI API → OpenAI
  AFTER:  <code> → virtual key → LiteLLM proxy → real OpenAI key → OpenAI

  Key Benefits:
  - Environment-based tracking via virtual keys (dev, test, eval, prod)
  - Component-based tracking via OTEL metadata
  - Centralized observability through Phoenix integration
  - Cost and rate limiting per virtual key

  Documentation to Review:
  - LiteLLM Docker Quick Start: https://docs.litellm.ai/docs/proxy/docker_quick_start
  - LiteLLM Virtual Keys: https://docs.litellm.ai/docs/proxy/virtual_keys
  - LiteLLM Configuration: https://docs.litellm.ai/docs/proxy/configs
  - LiteLLM OTEL Integration: https://docs.litellm.ai/docs/proxy/config_settings (search for "otel" or "callbacks")

  ---
  Milestone 1: LiteLLM Service Infrastructure

  Goal

  Set up LiteLLM proxy service with PostgreSQL backend in Docker Compose, ensuring it starts cleanly and can be reached by other services.

  Success Criteria

  - LiteLLM container starts successfully with database connection
  - Health check passes: curl http://localhost:4000/health
  - LiteLLM dashboard accessible at http://localhost:4000
  - PostgreSQL database postgres-litellm runs on separate port (5433)
  - Can view LiteLLM logs: docker compose logs litellm

  Key Changes

  1. Create config/litellm_config.yaml

  WHY: LiteLLM needs configuration for models, database, and OTEL integration.

  BEFORE IMPLEMENTING: Read https://docs.litellm.ai/docs/proxy/configs to understand all available options.

  # Model definitions - maps model names to OpenAI backend
  model_list:
    - model_name: gpt-4o
      litellm_params:
        model: openai/gpt-4o
        api_key: os.environ/OPENAI_API_KEY
        num_retries: 3
        timeout: 600  # 10 minutes for long requests

    - model_name: gpt-4o-mini
      litellm_params:
        model: openai/gpt-4o-mini
        api_key: os.environ/OPENAI_API_KEY
        num_retries: 3
        timeout: 600

    - model_name: o1-mini
      litellm_params:
        model: openai/o1-mini
        api_key: os.environ/OPENAI_API_KEY
        num_retries: 3
        timeout: 600

  # Proxy authentication and database
  general_settings:
    master_key: os.environ/LITELLM_MASTER_KEY
    database_url: "postgresql://litellm_user:${LITELLM_POSTGRES_PASSWORD}@postgres-litellm:5432/litellm"
    store_model_in_db: true  # Persist model config
    set_verbose: true  # Detailed logging

  # CRITICAL: OTEL integration for Phoenix observability
  litellm_settings:
    callbacks: ["otel"]
    success_callback: ["otel"]
    failure_callback: ["otel"]

    # VERIFY THIS IN DOCS: Does LiteLLM auto-discover Phoenix endpoint?
    # If not, add: otel_collector_endpoint: "http://phoenix:4317"
    # Read: https://docs.litellm.ai/docs/proxy/config_settings and search for "otel"

  QUESTION FOR HUMAN: After reading LiteLLM docs, verify if otel_collector_endpoint needs to be explicitly set or if LiteLLM auto-discovers from environment variables.

  2. Update docker-compose.yml

  WHY: Need separate PostgreSQL for LiteLLM and rename existing postgres service for clarity.

  Key changes:
  - Rename postgres service → postgres-phoenix (breaking change, acceptable)
  - Add new postgres-litellm service on port 5433
  - Add litellm service that depends on postgres-litellm
  - Update phoenix service dependency: postgres → postgres-phoenix
  - Update volume name: phoenix_postgres_data (keep same name to preserve existing data)
  - Add new volume: litellm_postgres_data

  Health check pattern for LiteLLM:
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:4000/health/readiness"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 30s

  WHY /health/readiness? Includes database connectivity check (read LiteLLM docs to verify available endpoints).

  3. Add environment variables to .env.dev.example

  Add new section at the top (before SHARED CONFIGURATION):

  # =============================================================================
  # LITELLM PROXY CONFIGURATION
  # =============================================================================

  # LiteLLM Master Key (for admin operations like creating virtual keys)
  # Used by: scripts/setup_litellm_keys.sh, LiteLLM proxy
  # Generate: python -c "import secrets; print('sk-' + secrets.token_urlsafe(32))"
  LITELLM_MASTER_KEY=

  # LiteLLM PostgreSQL Password
  # Used by: postgres-litellm service, LiteLLM database_url
  LITELLM_POSTGRES_PASSWORD=

  # LiteLLM Virtual Keys (generated by setup script - LEAVE EMPTY initially)
  # After running `make litellm_setup`, copy generated keys here

  # LITELLM_API_KEY: Virtual key for development (used by reasoning-api)
  LITELLM_API_KEY=

  # LITELLM_TEST_KEY: Virtual key for integration tests (with rate limits)
  LITELLM_TEST_KEY=

  # LITELLM_EVAL_KEY: Virtual key for LLM evaluations
  LITELLM_EVAL_KEY=

  # =============================================================================
  # SHARED CONFIGURATION
  # =============================================================================

  # CRITICAL BEHAVIOR NOTE:
  # In Docker dev mode (docker-compose.dev.yml), the reasoning-api container
  # receives LITELLM_API_KEY as OPENAI_API_KEY (automatic override).
  # The LiteLLM proxy uses the REAL OPENAI_API_KEY to call OpenAI.
  #
  # Flow: reasoning-api → LITELLM_API_KEY → litellm proxy → OPENAI_API_KEY → OpenAI

  # OpenAI API Configuration
  # Required: Your REAL OpenAI API key (used by LiteLLM proxy)
  # Get from: https://platform.openai.com/api-keys
  OPENAI_API_KEY=your-openai-api-key-here

  # ... rest of existing config

  Update workflow section:
  # =============================================================================
  # DEVELOPMENT WORKFLOW
  # =============================================================================

  # 1. Copy this file: cp .env.dev.example .env
  # 2. Add your OpenAI API key to OPENAI_API_KEY
  # 3. Generate LiteLLM master key: python -c "import secrets; print('sk-' + secrets.token_urlsafe(32))"
  # 4. Set LITELLM_MASTER_KEY and LITELLM_POSTGRES_PASSWORD
  # 5. Start services: docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d
  # 6. Generate virtual keys: make litellm_setup
  # 7. Copy generated keys to LITELLM_API_KEY, LITELLM_TEST_KEY, LITELLM_EVAL_KEY in .env
  # 8. Restart reasoning-api: docker compose restart reasoning-api
  # 9. Access web interface: http://localhost:8080
  # 10. Access API docs: http://localhost:8000/docs
  # 11. Access Phoenix UI: http://localhost:6006
  # 12. Access LiteLLM dashboard: http://localhost:4000

  Testing Strategy

  Manual verification (no automated tests for this milestone):
  1. Start services: docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d
  2. Check all services healthy: docker compose ps
  3. Verify LiteLLM logs show successful startup: docker compose logs litellm
  4. Check database connection: Look for "Connected to database" in logs
  5. Access LiteLLM dashboard: http://localhost:4000 (should show login or models page)
  6. Verify Phoenix still works: http://localhost:6006
  7. Check postgres-phoenix on 5432: docker compose exec postgres-phoenix pg_isready
  8. Check postgres-litellm on 5433: docker compose exec postgres-litellm pg_isready

  Expected errors to resolve:
  - LiteLLM might fail to start if database_url format is wrong
  - Phoenix might fail if we broke the postgres dependency
  - Port conflicts if 4000 or 5433 already in use

  Dependencies

  None - this is the foundation.

  Risk Factors

  1. OTEL endpoint configuration - May need explicit endpoint setting (verify in docs)
  2. Database schema initialization - LiteLLM should auto-create schema, but verify in logs
  3. Port conflicts - Developer may have services on 4000 or 5433
  4. Volume naming - Ensure phoenix data is preserved after postgres rename

  ---
  Milestone 2: Virtual Key Generation Script

  Goal

  Create automated script to generate virtual keys for different use cases (dev, test, eval) with proper health checks and error handling.

  Success Criteria

  - Script waits for LiteLLM to be ready before creating keys
  - Creates 3 virtual keys with different configurations
  - Outputs keys in copy-pasteable format for .env
  - Script is executable: chmod +x scripts/setup_litellm_keys.sh
  - make litellm_setup successfully generates keys
  - Keys are visible in LiteLLM dashboard at http://localhost:4000

  Key Changes

  1. Create scripts/setup_litellm_keys.sh

  WHY: Manual key creation is error-prone and not reproducible. Script ensures consistent setup.

  BEFORE IMPLEMENTING: Read https://docs.litellm.ai/docs/proxy/virtual_keys to understand the /key/generate API endpoint and available parameters.

  Key requirements:
  - Wait for LiteLLM health check (max 30 retries, 2 seconds each)
  - Use LiteLLM master key for authentication
  - Create 3 keys with different configurations:
    - litellm-dev: No limits, for development
    - litellm-test: max_budget: 1.0, rpm_limit: 10 (prevent runaway test costs)
    - litellm-eval: No limits, for evaluations
  - Output keys in format ready to paste into .env
  - Include error handling and validation

  Pattern for key creation:
  # Example API call (verify exact endpoint and payload in docs)
  curl -X POST 'http://localhost:4000/key/generate' \
    -H 'Authorization: Bearer ${LITELLM_MASTER_KEY}' \
    -H 'Content-Type: application/json' \
    -d '{
      "key_alias": "litellm-dev",
      "models": ["gpt-4o", "gpt-4o-mini", "o1-mini"],
      "metadata": {"environment": "development"}
    }'

  QUESTION FOR HUMAN: Verify the exact API endpoint and payload format from LiteLLM docs. The example above may need adjustment.

  2. Add Makefile targets

  Add to Makefile:
  .PHONY: litellm_setup
  litellm_setup: ## Setup LiteLLM virtual keys (run after docker compose up)
      @echo "Setting up LiteLLM virtual keys..."
      @./scripts/setup_litellm_keys.sh

  .PHONY: litellm_ui
  litellm_ui: ## Open LiteLLM dashboard in browser
      @echo "Opening LiteLLM dashboard..."
      @open http://localhost:4000 || xdg-open http://localhost:4000 || echo "Please open http://localhost:4000 in your browser"

  .PHONY: litellm_logs
  litellm_logs: ## Show LiteLLM proxy logs
      @docker compose logs -f litellm

  .PHONY: litellm_restart
  litellm_restart: ## Restart LiteLLM service
      @docker compose restart litellm

  .PHONY: litellm_reset
  litellm_reset: ## Reset LiteLLM database and regenerate keys (DESTRUCTIVE)
      @echo "WARNING: This will delete all virtual keys and usage data!"
      @echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
      @sleep 5
      @docker compose stop litellm
      @docker volume rm reasoning-agent-api_litellm_postgres_data || true
      @docker compose up -d litellm
      @echo "Waiting for LiteLLM to initialize..."
      @sleep 10
      @make litellm_setup

  Update help section to include new targets.

  Testing Strategy

  Manual verification:
  1. Start services: docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d
  2. Run script: make litellm_setup
  3. Verify output shows 3 keys generated
  4. Copy keys to .env file
  5. Check LiteLLM dashboard shows 3 keys with correct metadata
  6. Test each key with curl:
  curl -X POST 'http://localhost:4000/chat/completions' \
    -H 'Authorization: Bearer <LITELLM_API_KEY>' \
    -H 'Content-Type: application/json' \
    -d '{
      "model": "gpt-4o-mini",
      "messages": [{"role": "user", "content": "Hello!"}]
    }'

  Error cases to test:
  - Run script when LiteLLM is not running (should fail gracefully)
  - Run script with invalid master key (should show clear error)
  - Run script twice (should handle duplicate key names)

  Dependencies

  - Milestone 1 complete (LiteLLM service running)

  Risk Factors

  1. API endpoint changes - LiteLLM docs may show different endpoint than expected
  2. Master key format - Must start with sk- or LiteLLM may reject
  3. Rate limit configuration - Test key limits may need tuning based on test suite behavior

  ---
  Milestone 3: Configuration Changes (BREAKING CHANGES)

  Goal

  Update application configuration to use LiteLLM proxy instead of direct OpenAI API, including renaming configuration fields for clarity.

  Success Criteria

  - api/config.py has litellm_base_url field (replaces reasoning_agent_base_url)
  - Default value points to LiteLLM: http://litellm:4000
  - All references to old field name are updated
  - Configuration loads successfully in tests
  - No direct OpenAI API calls remain in codebase

  Key Changes

  1. Update api/config.py

  WHY: Renaming reasoning_agent_base_url → litellm_base_url makes the configuration clearer and indicates the architectural change.

  BREAKING CHANGE: Existing .env files with REASONING_AGENT_BASE_URL will need to use LITELLM_BASE_URL instead.

  # OLD (remove this):
  reasoning_agent_base_url: str = Field(
      default="https://api.openai.com/v1",
      alias="REASONING_AGENT_BASE_URL",
  )

  # NEW (add this):
  litellm_base_url: str = Field(
      default="http://litellm:4000",
      alias="LITELLM_BASE_URL",
      description="Base URL for LiteLLM proxy (routes to OpenAI)",
  )

  NOTE: The default changes from OpenAI direct to LiteLLM proxy. This is intentional - production deployments should always use LiteLLM.

  2. Update api/dependencies.py

  WHY: ReasoningAgent needs to use the new configuration field.

  # In get_reasoning_agent() function:
  # OLD:
  return ReasoningAgent(
      base_url=settings.reasoning_agent_base_url,  # ← Remove
      ...
  )

  # NEW:
  return ReasoningAgent(
      base_url=settings.litellm_base_url,  # ← Use new field
      ...
  )

  3. Update api/passthrough.py - CRITICAL

  WHY: Passthrough path creates its own AsyncOpenAI client. This MUST use LiteLLM or it bypasses the entire proxy layer.

  CURRENT CODE (line 76-80):
  openai_client = AsyncOpenAI(
      api_key=settings.openai_api_key,
      base_url=settings.reasoning_agent_base_url,  # ← OLD
  )

  NEW CODE:
  openai_client = AsyncOpenAI(
      api_key=settings.openai_api_key,  # This is virtual key in production
      base_url=settings.litellm_base_url,  # ← NEW
  )

  4. Update api/request_router.py - CRITICAL

  WHY: Request router creates AsyncOpenAI client for LLM classifier. This MUST use LiteLLM.

  CURRENT CODE (line 279-282):
  openai_client = AsyncOpenAI(
      api_key=settings.openai_api_key,
      base_url=settings.reasoning_agent_base_url,  # ← OLD
  )

  NEW CODE:
  openai_client = AsyncOpenAI(
      api_key=settings.openai_api_key,  # This is virtual key in production
      base_url=settings.litellm_base_url,  # ← NEW
  )

  5. Update .env.dev.example and .env.prod.example

  WHY: Documentation needs to reflect the new configuration.

  Changes needed:
  - Remove all references to REASONING_AGENT_BASE_URL
  - Add documentation for LITELLM_BASE_URL
  - Clarify default behavior

  # Remove this old section:
  # REASONING_AGENT_BASE_URL: Base URL for OpenAI API or compatible service
  # REASONING_AGENT_BASE_URL=https://api.openai.com/v1

  # Add this new section:
  # LiteLLM Proxy Configuration (set automatically in Docker)
  # LITELLM_BASE_URL: Base URL for LiteLLM proxy
  # Default: http://litellm:4000 (Docker internal networking)
  # Override only if running LiteLLM on different host/port
  # LITELLM_BASE_URL=http://litellm:4000

  6. Update docker-compose.yml

  WHY: Container needs to know about LiteLLM base URL.

  reasoning-api:
    environment:
      # ... existing vars ...

      # Remove (old):
      # - REASONING_AGENT_BASE_URL=${REASONING_AGENT_BASE_URL:-https://api.openai.com/v1}

      # Add (new):
      - LITELLM_BASE_URL=http://litellm:4000

  7. Update docker-compose.dev.yml

  WHY: Development environment overrides OPENAI_API_KEY with virtual key.

  CRITICAL: This is how the virtual key gets injected in development.

  reasoning-api:
    environment:
      # ... existing dev vars ...

      # Override OPENAI_API_KEY with virtual key in dev mode
      # The application code uses settings.openai_api_key, which now receives the virtual key
      # The REAL OpenAI key is used by the LiteLLM proxy
      - OPENAI_API_KEY=${LITELLM_API_KEY}

      # LiteLLM base URL (explicit for clarity, already set in base compose file)
      - LITELLM_BASE_URL=http://litellm:4000

  Testing Strategy

  Unit Tests

  Update existing tests that reference reasoning_agent_base_url:

  # Find all tests that use settings.reasoning_agent_base_url
  # Update to use settings.litellm_base_url

  # Example in tests/unit_tests/test_config.py:
  def test_config_defaults():
      settings = Settings()
      assert settings.litellm_base_url == "http://litellm:4000"  # New default

  Search for references:
  grep -r "reasoning_agent_base_url" tests/
  grep -r "REASONING_AGENT_BASE_URL" tests/

  Update all occurrences.

  Integration Tests (addressed in Milestone 4)

  For now, tests will continue using direct OpenAI. Milestone 4 will add LiteLLM support.

  Dependencies

  - Milestone 1 complete (LiteLLM service available)
  - Milestone 2 complete (virtual keys generated)

  Risk Factors

  1. Breaking change impact - Existing deployments need .env updates
  2. Test failures - Tests hardcoded to use OpenAI direct may break
  3. Client code assumptions - Code assuming settings.reasoning_agent_base_url will break

  MITIGATION: This is acceptable per requirements ("no backwards compatibility required"). Focus on clean design.

  ---
  Milestone 4: Test Configuration Strategy

  Goal

  Enable integration tests to run with either LiteLLM proxy OR direct OpenAI, configurable via environment variable.

  Success Criteria

  - Integration tests run successfully with direct OpenAI (default)
  - Integration tests run successfully with LiteLLM proxy (when enabled)
  - Test configuration is documented in .env.dev.example
  - Tests use appropriate API key based on configuration
  - No test failures introduced by LiteLLM migration

  Key Changes

  1. Add test configuration to api/config.py

  WHY: Tests need a way to bypass LiteLLM for faster execution, but also support testing through LiteLLM.

  class Settings(BaseSettings):
      # ... existing fields ...

      # Test configuration
      integration_tests_use_litellm: bool = Field(
          default=False,
          alias="INTEGRATION_TESTS_USE_LITELLM",
          description="Whether integration tests should route through LiteLLM proxy",
      )
      litellm_test_key: str = Field(
          default="",
          alias="LITELLM_TEST_KEY",
          description="Virtual key for integration tests (when using LiteLLM)",
      )

  2. Update test fixtures in tests/conftest.py

  WHY: Test fixtures create ReasoningAgent instances with hardcoded URLs. Need to make this configurable.

  Current pattern (line 88-93):
  agent = ReasoningAgent(
      base_url="https://api.openai.com/v1",  # ← Hardcoded
      api_key=os.getenv("OPENAI_API_KEY"),
      ...
  )

  New pattern:
  from api.config import settings

  # Determine base URL and API key based on test configuration
  if settings.integration_tests_use_litellm:
      base_url = settings.litellm_base_url
      api_key = settings.litellm_test_key or os.getenv("LITELLM_TEST_KEY")
  else:
      base_url = "https://api.openai.com/v1"
      api_key = os.getenv("OPENAI_API_KEY")

  agent = ReasoningAgent(
      base_url=base_url,
      api_key=api_key,
      ...
  )

  3. Update .env.dev.example

  Add test configuration section:

  # =============================================================================
  # INTEGRATION TESTS CONFIGURATION
  # =============================================================================

  # INTEGRATION_TESTS_USE_LITELLM: Route integration tests through LiteLLM proxy
  # - false (default): Tests call OpenAI directly (faster, simpler)
  # - true: Tests call through LiteLLM proxy (tests the full production path)
  #
  # When false: Tests use OPENAI_API_KEY directly
  # When true: Tests use LITELLM_TEST_KEY (must run `make litellm_setup` first)
  INTEGRATION_TESTS_USE_LITELLM=false

  # Note: LITELLM_TEST_KEY is already defined in LiteLLM section above

  4. Add documentation to CLAUDE.md

  Update test organization section:

  ### Test Configuration

  Integration tests can run in two modes:

  1. **Direct OpenAI (default):** Faster, simpler, bypasses LiteLLM
     - Set `INTEGRATION_TESTS_USE_LITELLM=false` (or leave unset)
     - Uses `OPENAI_API_KEY` directly

  2. **Via LiteLLM proxy:** Tests full production path
     - Set `INTEGRATION_TESTS_USE_LITELLM=true`
     - Uses `LITELLM_TEST_KEY` (must run `make litellm_setup` first)
     - Requires LiteLLM service running: `make docker_up`

  Example:
  ```bash
  # Test with direct OpenAI (fast)
  make integration_tests

  # Test through LiteLLM (full production path)
  INTEGRATION_TESTS_USE_LITELLM=true make integration_tests

  ### Testing Strategy

  #### Manual verification:
  1. **Test with direct OpenAI (default behavior):**
     ```bash
     make integration_tests
  - Should pass without LiteLLM running
  - Should use OPENAI_API_KEY from .env

  2. Test with LiteLLM proxy:
  # Start LiteLLM
  docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d

  # Set configuration
  export INTEGRATION_TESTS_USE_LITELLM=true

  # Run tests
  make integration_tests
    - Should route through LiteLLM on port 4000
    - Should use LITELLM_TEST_KEY
    - Check LiteLLM logs for request activity: make litellm_logs
  3. Verify test isolation:
    - Run make litellm_reset to clear usage data
    - Run tests through LiteLLM
    - Check LiteLLM dashboard for usage metrics
    - Verify rate limits are enforced (test key has rpm_limit: 10)

  Edge cases to test:

  - Tests fail gracefully if INTEGRATION_TESTS_USE_LITELLM=true but LiteLLM not running
  - Tests fail gracefully if LITELLM_TEST_KEY is missing
  - Test key rate limits don't cause test failures (may need to adjust limits)

  Dependencies

  - Milestone 3 complete (configuration changes applied)
  - LiteLLM service running (for LiteLLM mode tests)

  Risk Factors

  1. Rate limits too restrictive - Test key rpm_limit: 10 may be too low for test suite
  2. Test timing assumptions - Tests may assume certain latency from direct OpenAI
  3. Fixture complexity - Adding conditional logic to fixtures may make them harder to maintain

  MITIGATION: Start with direct OpenAI as default (minimal impact). Add LiteLLM mode as opt-in.

  ---
  Milestone 5: Component Tracking via OTEL Metadata

  Goal

  Add component-level tracking to LLM calls so Phoenix can distinguish between reasoning-agent, passthrough, classifier, etc.

  Success Criteria

  - All LLM calls include metadata with component name
  - Phoenix traces show component information in metadata
  - Can filter Phoenix traces by component
  - OTEL integration verified in LiteLLM logs

  Key Changes

  1. Research OTEL integration pattern

  BEFORE IMPLEMENTING: Read LiteLLM documentation to verify how to pass metadata to OTEL:
  - Check if metadata is passed via extra_body parameter
  - Check if there's a dedicated metadata parameter
  - Verify what fields get forwarded to OTEL/Phoenix

  Documentation to verify:
  - https://docs.litellm.ai/docs/proxy/config_settings (search for "metadata" and "otel")
  - https://docs.litellm.ai/docs/observability/callbacks (OTEL callback documentation)

  QUESTION FOR HUMAN: After reading docs, confirm the correct pattern for passing component metadata.

  2. Update api/reasoning_agent.py

  WHY: Add component tracking to reasoning step generation and synthesis.

  Pattern to implement (verify in docs):
  # In _generate_reasoning_step() method (around line 659):
  response = await self.openai_client.chat.completions.create(
      model=request.model,
      messages=messages,
      response_format={"type": "json_object"},
      temperature=request.temperature or DEFAULT_TEMPERATURE,
      # Add component metadata (verify exact parameter name in docs)
      metadata={
          "component": "reasoning-agent",
          "operation": "step_generation",
      },
  )

  # In _stream_final_synthesis() method (around line 524):
  stream = await self.openai_client.chat.completions.create(
      model=request.model,
      messages=messages,
      stream=True,
      temperature=request.temperature or DEFAULT_TEMPERATURE,
      stream_options={"include_usage": True},
      # Add component metadata
      metadata={
          "component": "reasoning-agent",
          "operation": "synthesis",
      },
  )

  3. Update api/passthrough.py

  WHY: Passthrough requests should be tagged differently from reasoning requests.

  # In execute_passthrough_stream() function (around line 87):
  stream = await openai_client.chat.completions.create(
      model=request.model,
      messages=request.messages,
      # ... other parameters ...
      stream=True,
      stream_options={"include_usage": True},
      # Add component metadata
      metadata={
          "component": "passthrough",
          "routing_path": "passthrough",
      },
  )

  4. Update api/request_router.py

  WHY: Classifier LLM calls should be tracked separately.

  # In _classify_with_llm() function (around line 285):
  response = await openai_client.chat.completions.parse(
      model=settings.routing_classifier_model,
      messages=[...],
      temperature=settings.routing_classifier_temperature,
      response_format=ClassifierRoutingDecision,
      # Add component metadata
      metadata={
          "component": "routing-classifier",
          "operation": "auto_routing",
      },
  )

  5. Update config/litellm_config.yaml if needed

  IF research shows explicit OTEL configuration is needed:

  litellm_settings:
    callbacks: ["otel"]
    success_callback: ["otel"]
    failure_callback: ["otel"]

    # Add if needed (verify in docs):
    otel_collector_endpoint: "http://phoenix:4317"
    service_name: "litellm-proxy"

    # Metadata forwarding (verify exact configuration):
    forward_metadata_to_otel: true  # or similar setting

  Testing Strategy

  Manual verification:

  1. Start services with LiteLLM: make docker_up
  2. Make a request through reasoning path:
  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "X-Routing-Mode: reasoning" \
    -d '{
      "model": "gpt-4o-mini",
      "messages": [{"role": "user", "content": "What is 2+2?"}]
    }'
  3. Check Phoenix UI (http://localhost:6006):
    - Find the trace for this request
    - Verify metadata includes "component": "reasoning-agent"
    - Verify there are spans for both step generation and synthesis
  4. Make a request through passthrough:
  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "X-Routing-Mode: passthrough" \
    -d '{
      "model": "gpt-4o-mini",
      "messages": [{"role": "user", "content": "Hello!"}]
    }'
    - Verify metadata includes "component": "passthrough"
  5. Check LiteLLM logs for OTEL activity:
  make litellm_logs | grep -i otel

  Edge cases:

  - Verify metadata works with streaming responses
  - Verify metadata is preserved across multiple LLM calls in one request
  - Check that missing metadata doesn't break requests (graceful degradation)

  Dependencies

  - Milestone 1 complete (LiteLLM with OTEL configured)
  - Milestone 3 complete (all code using LiteLLM)

  Risk Factors

  1. Metadata parameter name - May be metadata, extra_body, or something else
  2. OTEL forwarding - LiteLLM may not automatically forward metadata to Phoenix
  3. Breaking changes - Adding metadata parameter may cause errors if format is wrong

  MITIGATION: Thoroughly read LiteLLM docs before implementing. Test with simple curl requests first.

  ---
  Milestone 6: Production Configuration and Documentation

  Goal

  Create production-ready configuration examples and comprehensive documentation for deployment.

  Success Criteria

  - .env.prod.example has complete LiteLLM configuration
  - CLAUDE.md documents LiteLLM architecture and usage
  - README.md includes LiteLLM setup instructions
  - All Makefile targets documented in help text
  - Production deployment guide includes LiteLLM setup

  Key Changes

  1. Update .env.prod.example

  Add complete LiteLLM section:

  # =============================================================================
  # LITELLM PROXY CONFIGURATION
  # =============================================================================

  # LiteLLM Master Key (for admin operations)
  # CRITICAL: Generate a secure key for production!
  # Generate: python -c "import secrets; print('sk-' + secrets.token_urlsafe(32))"
  LITELLM_MASTER_KEY=

  # LiteLLM PostgreSQL Password
  # CRITICAL: Use a strong password for production!
  LITELLM_POSTGRES_PASSWORD=

  # LiteLLM Production Virtual Key
  # Generate this key in production by running: make litellm_setup
  # This key is used by all application components (reasoning-api, web-client, etc.)
  LITELLM_API_KEY=

  # =============================================================================
  # SHARED CONFIGURATION
  # =============================================================================

  # OpenAI API Configuration
  # Required: Your REAL OpenAI API key (used by LiteLLM proxy to call OpenAI)
  # Get from: https://platform.openai.com/api-keys
  # IMPORTANT: This is NOT the key your application uses - applications use LITELLM_API_KEY
  OPENAI_API_KEY=

  # ... rest of config

  Add production notes section:

  # =============================================================================
  # PRODUCTION DEPLOYMENT NOTES
  # =============================================================================

  # LiteLLM Architecture:
  # - Application code uses LITELLM_API_KEY (virtual key for tracking)
  # - LiteLLM proxy uses OPENAI_API_KEY (real key for API calls)
  # - All LLM requests flow through LiteLLM for observability and cost control
  #
  # Request Flow:
  # reasoning-api → LITELLM_API_KEY → litellm proxy → OPENAI_API_KEY → OpenAI
  #
  # Benefits:
  # - Unified observability in Phoenix (all requests traced)
  # - Cost tracking per environment via virtual keys
  # - Rate limiting and budgets per virtual key
  # - Easy to add new LLM providers (Azure, Anthropic, etc.)
  #
  # Services:
  # - LiteLLM Dashboard: http://your-domain:4000
  # - Phoenix Dashboard: http://your-domain:6006
  # - Reasoning API: http://your-domain:8000

  # Virtual Key Management:
  # 1. Generate production key: make litellm_setup
  # 2. Copy LITELLM_API_KEY to this file
  # 3. Never commit real keys to version control!
  # 4. Rotate keys periodically for security
  # 5. Monitor usage in LiteLLM dashboard

  2. Update CLAUDE.md

  Add LiteLLM architecture section after "Architecture Overview":

  ### LiteLLM Proxy Integration

  **Architecture:**
  ┌─────────────────────────────────────────────────────────────────┐
  │                      LLM Request Flow                            │
  └─────────────────────────────────────────────────────────────────┘

  Application Code → Virtual Key → LiteLLM Proxy → Real OpenAI Key → OpenAI API

  Components:
  - reasoning-agent: Uses virtual key for reasoning LLM calls
  - passthrough: Uses virtual key for direct OpenAI forwarding
  - request-router: Uses virtual key for routing classification
  - web-client: Uses virtual key for any direct LLM calls

  **Key Concepts:**

  - **Virtual Keys:** Environment-specific API keys (dev, test, prod) tracked by LiteLLM
  - **Real Key:** Single OpenAI API key used by LiteLLM proxy
  - **Component Metadata:** OTEL metadata tags LLM calls by component (reasoning vs passthrough)
  - **Unified Observability:** All LLM requests traced in Phoenix with cost/usage data

  **Configuration:**

  - `LITELLM_BASE_URL`: LiteLLM proxy endpoint (default: `http://litellm:4000`)
  - `OPENAI_API_KEY`: In production, this is the VIRTUAL key (not real OpenAI key)
  - Real OpenAI key is configured in LiteLLM service environment

  **Local Development:**

  ```bash
  # 1. Start services (includes LiteLLM)
  make docker_up

  # 2. Generate virtual keys
  make litellm_setup

  # 3. Copy keys to .env
  # LITELLM_API_KEY=sk-...
  # LITELLM_TEST_KEY=sk-...
  # LITELLM_EVAL_KEY=sk-...

  # 4. Restart reasoning-api to load new keys
  docker compose restart reasoning-api

  # 5. Access dashboards
  make litellm_ui  # LiteLLM dashboard
  # Phoenix: http://localhost:6006

  Monitoring:

  - LiteLLM Dashboard (http://localhost:4000): Virtual keys, usage, rate limits
  - Phoenix Dashboard (http://localhost:6006): Request traces, latency, errors

  Integration Tests:

  By default, integration tests bypass LiteLLM for speed. To test through LiteLLM:

  INTEGRATION_TESTS_USE_LITELLM=true make integration_tests

  #### 3. Update `README.md`

  Add LiteLLM section to architecture documentation (if not already present). Update quick start to mention LiteLLM setup.

  #### 4. Create `docs/litellm-setup.md` (optional)

  Comprehensive guide for LiteLLM setup, troubleshooting, and best practices.

  ### Testing Strategy

  **Documentation verification:**
  1. Follow setup instructions in `.env.prod.example` from scratch
  2. Verify all Makefile commands work as documented
  3. Check that CLAUDE.md accurately reflects current architecture
  4. Ensure README quick start works for new developers

  **No automated tests for this milestone** - focus on documentation quality.

  ### Dependencies
  - All previous milestones complete
  - Working LiteLLM integration

  ### Risk Factors
  1. **Documentation drift** - Docs may become outdated as code evolves
  2. **Platform-specific instructions** - Commands may differ on Windows/Mac/Linux

  **MITIGATION:** Keep documentation close to code (CLAUDE.md is checked in). Test instructions on clean environment.

  ---

  ## Post-Implementation Verification

  After all milestones are complete, perform comprehensive verification:

  ### 1. End-to-End Test (Manual)
  ```bash
  # Start from clean state
  make docker_down
  docker volume prune -f

  # Set up from scratch
  cp .env.dev.example .env
  # Edit .env: add OPENAI_API_KEY, generate LITELLM_MASTER_KEY, set passwords

  # Start services
  make docker_up

  # Generate keys
  make litellm_setup

  # Copy keys to .env
  # Edit .env: paste LITELLM_API_KEY, LITELLM_TEST_KEY, LITELLM_EVAL_KEY

  # Restart reasoning-api
  docker compose restart reasoning-api

  # Test all paths
  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "X-Routing-Mode: passthrough" \
    -d '{"model": "gpt-4o-mini", "messages": [{"role": "user", "content": "Test passthrough"}]}'

  curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "X-Routing-Mode: reasoning" \
    -d '{"model": "gpt-4o-mini", "messages": [{"role": "user", "content": "Test reasoning"}]}'

  # Verify in Phoenix
  # - Check traces appear
  # - Check metadata includes component tags
  # - Check usage data is tracked

  # Verify in LiteLLM Dashboard
  # - Check virtual keys show usage
  # - Check rate limits are working (if applicable)

  2. Integration Test Suite

  # Test with direct OpenAI (default)
  make integration_tests

  # Test through LiteLLM
  INTEGRATION_TESTS_USE_LITELLM=true make integration_tests

  3. Observability Verification

  - Phoenix shows traces from all components (reasoning, passthrough, classifier)
  - Metadata includes correct component tags
  - LiteLLM dashboard shows usage per virtual key
  - OTEL traces include LiteLLM spans

  4. Performance Check

  - Passthrough latency is acceptable (should be ~same as direct OpenAI + network hop)
  - Reasoning path latency is acceptable
  - No connection pool exhaustion under load
  - LiteLLM proxy doesn't become bottleneck

  ---

  Questions for Review/Research

  Before starting implementation, please confirm:

  1. OTEL Endpoint: Does LiteLLM auto-discover Phoenix OTEL endpoint (http://phoenix:4317) or does it need explicit configuration in litellm_config.yaml?
  2. Metadata Format: What is the exact parameter name for passing component metadata to LiteLLM? (e.g., metadata={}, extra_body={"metadata": {}}, etc.)
  3. Database Schema: Does LiteLLM auto-create its PostgreSQL schema on first startup, or do we need an init script?
  4. Virtual Key API: What is the exact endpoint and payload format for creating virtual keys via API? (The plan assumes /key/generate but this should be verified in docs)
  5. Rate Limits: Is rpm_limit: 10 reasonable for the test key, or should it be higher/lower based on test suite behavior?
  6. Breaking Changes: Confirm that breaking .env changes (renaming REASONING_AGENT_BASE_URL → LITELLM_BASE_URL) are acceptable for this migration.
  7. Test Strategy: Is the approach of "direct OpenAI by default, LiteLLM opt-in" for integration tests acceptable, or should tests always use LiteLLM?

  ---
  Success Metrics

  After implementation is complete, verify:

  - ✅ All LLM requests route through LiteLLM proxy
  - ✅ Virtual keys provide environment-based tracking (dev/test/eval/prod)
  - ✅ Component metadata enables filtering by component (reasoning/passthrough/classifier)
  - ✅ Phoenix traces include complete request flow with LiteLLM spans
  - ✅ Cost tracking visible in LiteLLM dashboard
  - ✅ Integration tests pass in both direct and LiteLLM modes
  - ✅ Documentation is complete and accurate
  - ✅ Setup process is reproducible from clean state
  - ✅ No performance degradation vs direct OpenAI calls

  ---
  References

  - LiteLLM Docker Quick Start: https://docs.litellm.ai/docs/proxy/docker_quick_start
  - LiteLLM Virtual Keys: https://docs.litellm.ai/docs/proxy/virtual_keys
  - LiteLLM Configuration: https://docs.litellm.ai/docs/proxy/configs
  - LiteLLM Config Settings: https://docs.litellm.ai/docs/proxy/config_settings
  - LiteLLM Observability: https://docs.litellm.ai/docs/observability/callbacks

  ---
  This implementation plan provides a structured, milestone-based approach to integrating LiteLLM proxy while maintaining test coverage and documentation quality. Each milestone is independently
  verifiable and includes clear success criteria.

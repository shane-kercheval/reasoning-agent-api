# Implementation Plan: File-Based Prompt System

## Overview

Add support for defining prompts as Markdown files with YAML frontmatter, loaded from a configurable external directory. This enables prompts to be managed in a separate Git repository and edited without code changes.

**Key Design Decisions:**
- Format: Markdown files with YAML frontmatter (one file per prompt)
- Template engine: Jinja2 for variable substitution (with `ChainableUndefined` for silent empty strings)
- Location: Configurable directory via environment variable, mounted as Docker volume
- Class hierarchy: `BasePrompt` (ABC) → `PromptTemplate` (concrete, Jinja2-based)
- Coexistence: File-based (`PromptTemplate`) and code-based (`BasePrompt` subclasses) prompts both register in `PromptRegistry`
- Duplicate handling: Fail startup on duplicate prompt names (fail-fast)
- Type validation: Not implemented (Jinja2 handles coercion; caller responsible for sensible values)

**Architecture:**
- `BasePrompt` - Abstract base class, kept for future complex prompt types
- `PromptTemplate(BasePrompt)` - Concrete class for Jinja2 template-based prompts
- `PromptInfo` (renamed from `PromptDefinition`) - API response model for listing prompts
- `PromptResult` - Changed to return `content: str` instead of `messages: list`
- Parser function returns `PromptTemplate` instances directly (no intermediate dataclass)

---

## Milestone 0: Add Category Field to Tools and Prompts

### Goal
Add a `category: str | None` field to both tools and prompts for better organization.

### Success Criteria
- `ToolDefinition` model has `category: str | None` field (default `None`)
- `BaseTool` has `category` property (default `None`)
- All existing tool classes updated with appropriate categories
- `PromptDefinition` model has `category: str | None` field (will be renamed to `PromptInfo` in M1)
- `BasePrompt` has `category` property (default `None`)
- `GreetingPrompt` example updated with category
- API responses include category in tool/prompt listings
- Existing tests updated, new tests for category field

### Key Changes

**Update: `tools_api/tools_api/models.py`**

```python
class ToolDefinition(BaseModel):
    name: str = Field(description="Tool name (used in API endpoint)")
    description: str = Field(description="Human-readable description")
    parameters: dict[str, Any] = Field(description="JSON Schema for parameters")
    category: str | None = Field(default=None, description="Tool category for organization")
    tags: list[str] = Field(default_factory=list, description="Semantic tags")

class PromptDefinition(BaseModel):
    name: str = Field(description="Prompt name (used in API endpoint)")
    description: str = Field(description="Human-readable description")
    arguments: list[dict[str, Any]] = Field(...)
    category: str | None = Field(default=None, description="Prompt category for organization")
    tags: list[str] = Field(default_factory=list, description="Semantic tags")
```

**Update: `tools_api/tools_api/services/base.py`**

Add to both `BaseTool` and `BasePrompt`:

```python
@property
def category(self) -> str | None:
    """Category for organization. Override in subclasses."""
    return None
```

**Update: All existing tool classes**

Add `category` property to each tool class:
- Filesystem tools: `category = "filesystem"`
- GitHub/dev tools: `category = "github"`
- Web tools: `category = "web"`

**Update: `tools_api/tools_api/routers/tools.py` and `prompts.py`**

Include `category` in the list endpoint responses.

### Testing Strategy

- Verify `category` appears in `GET /tools/` response
- Verify `category` appears in `GET /prompts/` response
- Test default `None` category for tools/prompts that don't override
- Test category value for tools that do override

### Dependencies
None - this is foundational.

### Risk Factors
- Many files to update (all tool classes)
- Ensure no regressions in existing tests

---

## Milestone 1: Model Updates and Renames

### Goal
Update Pydantic models: rename `PromptDefinition` to `PromptInfo`, change `PromptResult` to return `content: str` instead of `messages: list`, and update `BasePrompt.render()` signature.

### Success Criteria
- `PromptDefinition` renamed to `PromptInfo` throughout codebase
- `PromptResult.messages` replaced with `PromptResult.content: str`
- `BasePrompt.render()` returns `str` instead of `list[dict[str, str]]`
- `GreetingPrompt` updated to return string
- All imports and usages updated
- Tests updated to reflect new return types

### Key Changes

**Update: `tools_api/tools_api/models.py`**

```python
class PromptResult(BaseModel):
    """Structured prompt rendering result."""
    success: bool = Field(description="Whether rendering was successful")
    content: str = Field(description="Rendered prompt content")
    error: str | None = Field(default=None, description="Error message if rendering failed")
    metadata: dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

class PromptInfo(BaseModel):  # Renamed from PromptDefinition
    """Prompt metadata for discovery."""
    name: str = Field(description="Prompt name (used in API endpoint)")
    description: str = Field(description="Human-readable description")
    arguments: list[dict[str, Any]] = Field(...)
    category: str | None = Field(default=None, description="Prompt category")
    tags: list[str] = Field(default_factory=list, description="Semantic tags")
```

**Update: `tools_api/tools_api/services/base.py`**

```python
class BasePrompt(ABC):
    @abstractmethod
    async def render(self, **kwargs) -> str:
        """Render prompt and return content string."""
        pass

    async def __call__(self, **kwargs) -> PromptResult:
        try:
            content = await self.render(**kwargs)
            return PromptResult(success=True, content=content)
        except Exception as e:
            return PromptResult(success=False, content="", error=str(e))
```

**Update: `tools_api/tools_api/services/prompts/example_prompt.py`**

Update `GreetingPrompt.render()` to return `str`:

```python
async def render(self, name: str, formal: bool = False, **kwargs) -> str:
    if formal:
        return f"Good day, {name}. How may I assist you today?"
    else:
        return f"Hey {name}! What can I help you with?"
```

**Update: `tools_api/tools_api/routers/prompts.py`**

Update import from `PromptDefinition` to `PromptInfo`.

### Testing Strategy

- All existing prompt tests updated for new return type
- Verify `POST /prompts/{name}` returns `{"success": true, "content": "...", ...}`
- Verify `GET /prompts/` returns `PromptInfo` objects

### Dependencies
- Milestone 0 (category field)

### Risk Factors
- Breaking change to API response format
- Need to update all tests that check `messages` field

---

## Milestone 2: PromptTemplate Class

### Goal
Create `PromptTemplate` class that extends `BasePrompt` and handles Jinja2 template rendering.

### Success Criteria
- `PromptTemplate` implements all `BasePrompt` abstract methods
- Constructor accepts individual fields: `name`, `description`, `arguments`, `tags`, `category`, `template`
- Optional `source_path` for better error messages
- Jinja2 rendering works with variable substitution and conditionals
- Missing required arguments raise clear errors (including source path if available)
- Optional arguments render as empty string if not provided (using `ChainableUndefined`)
- `PromptTemplate` instances work identically to code-based prompts in registry

### Key Changes

**New file: `tools_api/tools_api/services/prompts/template.py`**

Install Jinja2: `uv add jinja2`

Documentation: https://jinja.palletsprojects.com/en/3.1.x/

```python
from pathlib import Path
from typing import Any

from jinja2 import Environment, ChainableUndefined

from tools_api.services.base import BasePrompt


# Shared Jinja2 environment configured for lenient undefined handling
_jinja_env = Environment(undefined=ChainableUndefined)


class PromptTemplate(BasePrompt):
    """Prompt implementation using Jinja2 templates."""

    def __init__(
        self,
        name: str,
        description: str,
        template: str,
        arguments: list[dict[str, Any]] | None = None,
        category: str | None = None,
        tags: list[str] | None = None,
        source_path: Path | None = None,
    ):
        self._name = name
        self._description = description
        self._template_str = template
        self._template = _jinja_env.from_string(template)
        self._arguments = arguments or []
        self._category = category
        self._tags = tags or []
        self._source_path = source_path

    @property
    def name(self) -> str:
        return self._name

    @property
    def description(self) -> str:
        return self._description

    @property
    def arguments(self) -> list[dict[str, Any]]:
        return self._arguments

    @property
    def category(self) -> str | None:
        return self._category

    @property
    def tags(self) -> list[str]:
        return self._tags

    @property
    def source_path(self) -> Path | None:
        return self._source_path

    async def render(self, **kwargs) -> str:
        """Render template with provided arguments."""
        # Validate required arguments are present
        for arg in self._arguments:
            if arg.get("required", False) and arg["name"] not in kwargs:
                location = f" ({self._source_path})" if self._source_path else ""
                raise ValueError(
                    f"Missing required argument '{arg['name']}' for prompt '{self._name}'{location}"
                )

        # Render with Jinja2 (undefined variables become empty string via ChainableUndefined)
        return self._template.render(**kwargs)
```

**Jinja2 configuration notes:**
- Uses `ChainableUndefined` so undefined variables render as empty string (not raise errors)
- Required argument validation happens *before* rendering (explicit check)
- Template syntax errors surface at render time with clear messages
- `source_path` is optional and used only for error messages

**Update: `tools_api/tools_api/services/prompts/__init__.py`**

Export `PromptTemplate`:

```python
"""Prompt implementations."""

from tools_api.services.prompts.template import PromptTemplate

__all__ = ["PromptTemplate"]
```

### Testing Strategy

**File: `tools_api/tests/unit_tests/test_prompt_template.py`**

- Basic variable substitution `{{ name }}`
- Conditional blocks `{% if focus %}...{% endif %}`
- Missing required argument raises `ValueError` with argument name and prompt name
- Missing required argument includes source path in error if provided
- Optional argument omitted renders as empty string (not error)
- Optional argument provided works correctly
- All properties return correct values
- `__call__` wrapper returns `PromptResult` with correct structure
- Empty template renders to empty string
- Template with no variables renders as-is
- Unicode content in template and arguments
- Undefined variable in template renders as empty string (not error)

### Dependencies
- Milestone 1 (model updates, `render()` returns `str`)

### Risk Factors
- Jinja2 `ChainableUndefined` behavior - test thoroughly
- Template syntax errors at render time

---

## Milestone 3: Parser and Loader

### Goal
Create parser function that reads Markdown files with YAML frontmatter and returns `PromptTemplate` instances. Create loader that scans directories recursively and registers prompts.

### Success Criteria
- Parser extracts frontmatter fields: `name`, `description`, `arguments`, `tags`, `category`
- Parser extracts markdown body as template content
- Parser returns `PromptTemplate` instance directly (with `source_path` set)
- Parser validates required fields and raises clear errors
- Loader scans directory recursively for `*.md` files
- Invalid files log warnings but don't crash
- Non-existent directory raises `FileNotFoundError` (fail startup)
- Empty/missing directory config logs info and continues (no error)
- Duplicate prompt names raise errors (fail startup)

### Key Changes

**New file: `tools_api/tools_api/services/prompts/parser.py`**

Install: `uv add python-frontmatter`

Documentation: https://python-frontmatter.readthedocs.io/en/latest/

```python
from pathlib import Path

import frontmatter

from tools_api.services.prompts.template import PromptTemplate


def parse_prompt_file(file_path: Path) -> PromptTemplate:
    """
    Parse a markdown file with YAML frontmatter into a PromptTemplate.

    Required frontmatter fields: name, description
    Optional frontmatter fields: arguments, tags, category

    Args:
        file_path: Path to the markdown file

    Returns:
        PromptTemplate instance with source_path set

    Raises:
        ValueError: If required fields are missing or file is malformed
    """
    post = frontmatter.load(file_path)

    # Validate required fields
    if "name" not in post.metadata:
        raise ValueError(f"Missing required field 'name' in {file_path}")
    if "description" not in post.metadata:
        raise ValueError(f"Missing required field 'description' in {file_path}")

    return PromptTemplate(
        name=post.metadata["name"],
        description=post.metadata["description"],
        template=post.content,
        arguments=post.metadata.get("arguments", []),
        category=post.metadata.get("category"),  # None if not specified
        tags=post.metadata.get("tags", []),
        source_path=file_path,
    )
```

**New file: `tools_api/tools_api/services/prompts/loader.py`**

```python
import logging
from pathlib import Path

from tools_api.services.prompts.parser import parse_prompt_file
from tools_api.services.prompts.template import PromptTemplate
from tools_api.services.registry import PromptRegistry

logger = logging.getLogger(__name__)


def load_prompts_from_directory(directory: Path) -> list[PromptTemplate]:
    """
    Scan directory recursively for .md files and return PromptTemplate instances.

    Logs warnings for invalid files but continues loading others.

    Args:
        directory: Path to prompts directory

    Returns:
        List of successfully parsed PromptTemplate instances

    Raises:
        FileNotFoundError: If directory does not exist
    """
    if not directory.exists():
        raise FileNotFoundError(f"Prompts directory not found: {directory}")

    prompts = []
    for file_path in directory.rglob("*.md"):
        try:
            prompt = parse_prompt_file(file_path)
            prompts.append(prompt)
            logger.info(f"Loaded prompt '{prompt.name}' from {file_path}")
        except Exception as e:
            logger.warning(f"Failed to load prompt from {file_path}: {e}")

    return prompts


def register_prompts_from_directory(directory: Path) -> int:
    """
    Load prompts from directory and register in PromptRegistry.

    Duplicate prompt names will cause startup failure (fail-fast).

    Args:
        directory: Path to prompts directory

    Returns:
        Count of successfully registered prompts

    Raises:
        FileNotFoundError: If directory does not exist
        ValueError: On duplicate prompt names (fails startup)
    """
    prompts = load_prompts_from_directory(directory)
    for prompt in prompts:
        PromptRegistry.register(prompt)  # Raises ValueError on duplicate
    return len(prompts)
```

**Update: `tools_api/tools_api/services/prompts/__init__.py`**

Add exports:

```python
"""Prompt implementations."""

from tools_api.services.prompts.loader import (
    load_prompts_from_directory,
    register_prompts_from_directory,
)
from tools_api.services.prompts.parser import parse_prompt_file
from tools_api.services.prompts.template import PromptTemplate

__all__ = [
    "PromptTemplate",
    "parse_prompt_file",
    "load_prompts_from_directory",
    "register_prompts_from_directory",
]
```

**Update: `tools_api/tools_api/config.py`**

```python
class Settings(BaseSettings):
    # ... existing fields

    # External prompts directory (optional)
    prompts_directory: Path | None = None
```

**Update: `tools_api/tools_api/main.py`**

In lifespan function, replace hardcoded `GreetingPrompt` registration:

```python
from tools_api.services.prompts import register_prompts_from_directory

# Register file-based prompts from directory
if settings.prompts_directory:
    try:
        count = register_prompts_from_directory(settings.prompts_directory)
        logger.info(f"Registered {count} prompts from {settings.prompts_directory}")
    except FileNotFoundError as e:
        logger.error(f"Prompts directory not found: {e}")
        raise  # Fail startup if configured directory doesn't exist
    except ValueError as e:
        logger.error(f"Duplicate prompt name: {e}")
        raise  # Fail startup on duplicate prompt names
else:
    logger.info("No prompts directory configured, skipping file-based prompts")

# Note: GreetingPrompt is kept as example code but NOT registered
# See tools_api/services/prompts/example_prompt.py for code-based prompt example
```

**Expected file format:**

```markdown
---
name: code_review
description: Review code for quality and best practices
category: development
arguments:
  - name: language
    required: true
    description: Programming language of the code
  - name: focus
    required: false
    description: Specific areas to focus on
tags:
  - code
  - review
---
You are a senior {{ language }} developer performing a code review.

{% if focus %}
Focus specifically on: {{ focus }}
{% endif %}

Review the code for quality, bugs, and best practices.
```

### Testing Strategy

**File: `tools_api/tests/unit_tests/test_prompt_parser.py`**

Use `tmp_path` fixture to create test files.

- Valid file with all fields populated
- Valid file with optional fields omitted
- Missing required field (`name`) raises `ValueError`
- Missing required field (`description`) raises `ValueError`
- Malformed YAML raises error
- Empty template body (valid)
- File with no frontmatter raises error
- Unicode content works
- `source_path` is set on returned `PromptTemplate`

**File: `tools_api/tests/unit_tests/test_prompt_loader.py`**

- Directory with multiple valid `.md` files loads all
- Nested subdirectories are scanned (recursive)
- Non-`.md` files are ignored
- Invalid file logs warning and continues
- Empty directory returns empty list
- Non-existent directory raises `FileNotFoundError`
- Duplicate names across files raises `ValueError` during registration

### Dependencies
- Milestone 2 (`PromptTemplate` class)

### Risk Factors
- `python-frontmatter` library edge cases
- File permission errors in Docker

---

## Milestone 4: Integration Testing, Examples, and Docker Configuration

### Goal
End-to-end validation that file-based prompts work through the API. Create example prompt files for reference. Configure Docker volume mount for prompts directory.

### Success Criteria
- Integration test: prompts loaded from directory appear in `GET /prompts/`
- Integration test: prompts render correctly via `POST /prompts/{name}`
- Integration test: error handling for missing prompts, bad arguments
- Example prompt files demonstrate various features
- `GreetingPrompt` kept as code example but not registered
- Test fixtures directory for integration tests
- Docker Compose updated with prompts volume mount

### Key Changes

**New: `tools_api/tests/fixtures/prompts/`**

Create test prompt files for integration tests:

1. `simple.md` - Basic variable substitution
2. `conditional.md` - Conditional logic
3. `nested/categorized.md` - Test recursive loading

**Update: `tools_api/tests/integration_tests/conftest.py`**

Configure test fixtures directory:

```python
import os
from pathlib import Path

# Set prompts directory for integration tests before importing app
FIXTURES_DIR = Path(__file__).parent.parent / "fixtures" / "prompts"
os.environ["PROMPTS_DIRECTORY"] = str(FIXTURES_DIR)
```

**Update: `tools_api/tests/integration_tests/test_tools_api_http.py`**

Add tests:

```python
# Test that file-based prompts appear in listing
# Test rendering with required arguments
# Test rendering with optional arguments
# Test error on missing required argument
# Test 404 on non-existent prompt
```

**New: `tools_api/examples/prompts/`**

Create example prompt files for documentation:

1. `greeting.md` - Simple variable substitution
2. `code_review.md` - Conditionals and multiple arguments
3. `summarize.md` - Minimal example (required fields only)

**Update: `docker-compose.yml` (or `docker-compose.dev.yml`)**

Add prompts volume mount:

```yaml
services:
  tools-api:
    environment:
      - PROMPTS_DIRECTORY=/mnt/prompts
    volumes:
      # ... existing volumes
      - ./prompts:/mnt/prompts:ro  # read-only mount for prompts
```

**Update: `tools_api/README.md`**

Add section documenting:
- Prompt file format (frontmatter fields, template syntax)
- Jinja2 features supported (`{{ var }}`, `{% if %}`, `{% for %}`)
- Configuration via `PROMPTS_DIRECTORY` environment variable
- Docker volume mount configuration
- Category and tags usage
- Examples

**Update: `tools_api/tools_api/services/prompts/example_prompt.py`**

Add comment explaining this is an example of a code-based prompt for complex use cases, but is not registered by default:

```python
"""
Example code-based prompt for testing and demonstration.

This shows how to create a prompt class that extends BasePrompt for complex
use cases requiring custom logic. For simple template-based prompts, use
markdown files with YAML frontmatter instead (see examples/prompts/).

NOTE: This prompt is NOT registered by default. It serves as a reference
implementation for developers who need to create code-based prompts.
"""
```

### Testing Strategy

**Integration tests** (`tools_api/tests/integration_tests/`):

Use fixture directory at `tools_api/tests/fixtures/prompts/` with test prompt files.

- Start API with `PROMPTS_DIRECTORY` pointing to test fixtures (via conftest.py)
- Verify prompts appear in listing with correct metadata (name, description, category, tags)
- Verify rendering works with required arguments
- Verify rendering works with optional arguments (omitted → empty string)
- Verify error on missing required argument returns proper error response
- Verify 404 on non-existent prompt

### Dependencies
- Milestones 0-3

### Risk Factors
- Integration test setup complexity (env var must be set before app import)
- Ensuring test fixtures directory exists in CI
- Docker volume mount permissions

---

## Summary of Changes by File

| File | Changes |
|------|---------|
| `models.py` | Add `category` to `ToolDefinition`, rename `PromptDefinition` → `PromptInfo`, change `PromptResult.messages` → `content` |
| `base.py` | Add `category` property to `BaseTool` and `BasePrompt`, change `render()` return type to `str` |
| `registry.py` | No changes |
| `routers/prompts.py` | Update import to `PromptInfo` |
| `routers/tools.py` | Include `category` in response |
| `config.py` | Add `prompts_directory: Path \| None` |
| `main.py` | Replace hardcoded prompt registration with loader |
| `services/prompts/__init__.py` | Export `PromptTemplate`, parser, loader functions |
| `services/prompts/template.py` | New - `PromptTemplate` class with Jinja2 + `ChainableUndefined` |
| `services/prompts/parser.py` | New - `parse_prompt_file()` function |
| `services/prompts/loader.py` | New - `load_prompts_from_directory()`, `register_prompts_from_directory()` |
| `services/prompts/example_prompt.py` | Update `render()` return type, add documentation comments |
| `services/tools/*.py` | Add `category` property to all tool classes |
| `tests/fixtures/prompts/*.md` | New - test prompt files for integration tests |
| `tests/integration_tests/conftest.py` | Configure `PROMPTS_DIRECTORY` env var |
| `examples/prompts/*.md` | New - example prompt files for documentation |
| `docker-compose.yml` | Add prompts volume mount |
| `README.md` | Document prompt file format and Docker configuration |

---

## Dependencies (Libraries)

| Library | Version | Purpose |
|---------|---------|---------|
| `jinja2` | latest | Template rendering with `ChainableUndefined` |
| `python-frontmatter` | latest | YAML frontmatter parsing |

Install via: `uv add jinja2 python-frontmatter`

---

## Clarifications

**Jinja2 Undefined Behavior:**
- Uses `ChainableUndefined` from Jinja2 so undefined variables render as empty string
- Required argument validation is explicit (happens before Jinja2 rendering)
- This means: required args checked by our code, optional args handled silently by Jinja2

**Duplicate Prompt Names:**
- Duplicates cause startup failure (fail-fast)
- This is intentional - duplicates are configuration errors that should be caught immediately
- Silent skipping could hide bugs where the wrong prompt is used

**Argument Type Validation:**
- Not implemented - Jinja2 handles type coercion reasonably
- The caller (LLM/client) is responsible for passing sensible values
- Can be added later if needed

**Docker Volume Mount:**
- Prompts directory mounted read-only at `/mnt/prompts`
- Configure via `PROMPTS_DIRECTORY=/mnt/prompts` environment variable
- Host path configurable in docker-compose.yml

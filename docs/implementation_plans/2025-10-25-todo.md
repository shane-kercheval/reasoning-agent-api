- Change all code from using AsyncOpenAI to using litellm->acompletion - Using the litellm library (litellm.acompletion() / litellm.completion(), etc) means youâ€™re using their SDK which is designed to support multiple LLM providers, not just OpenAI. They wrap a variety of models and endpoints under a unified interface. 
    - There was a performance reason for creating AsyncOpenAI once and reusing it and adding it as a dependency. But with litellm, it looks like each call to litellm.acompletion() creates its own client internally. We need to research if there are any performance implications of this, or if litellm has its own internal optimizations.
- 
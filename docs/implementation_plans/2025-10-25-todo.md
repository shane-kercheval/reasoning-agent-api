- Change all code from using AsyncOpenAI to using litellm->acompletion - Using the litellm library (litellm.acompletion() / litellm.completion(), etc) means youâ€™re using their SDK which is designed to support multiple LLM providers, not just OpenAI. They wrap a variety of models and endpoints under a unified interface. 
    - There was a performance reason for creating AsyncOpenAI once and reusing it and adding it as a dependency. But with litellm, it looks like each call to litellm.acompletion() creates its own client internally. We need to research if there are any performance implications of this, or if litellm has its own internal optimizations.
    - litellm has an endpoint to return the available models, we need to expose that in our API as well so the client can dynamically display available models.
- We need to upgrade to python 3.14
- We need to update Ruff to the latest version and fix any new linting errors.
- Electron app creation
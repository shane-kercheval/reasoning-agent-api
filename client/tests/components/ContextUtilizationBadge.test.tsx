/**
 * Tests for ContextUtilizationBadge component.
 *
 * Critical tests that would have caught the double-multiplier bug
 * and visualization calculation errors.
 */

import { render, screen } from '@testing-library/react';
import { TooltipProvider } from '../../src/components/ui/tooltip';
import { ContextUtilizationBadge } from '../../src/components/chat/ContextUtilizationBadge';
import type { Usage } from '../../src/types/openai';

// Helper to wrap component in TooltipProvider
const renderWithProvider = (ui: React.ReactElement) => {
  return render(<TooltipProvider>{ui}</TooltipProvider>);
};

describe('ContextUtilizationBadge', () => {
  describe('calculation logic - prevents double-multiplier bug', () => {
    it('should NOT double-apply strategy multiplier for "low" strategy', () => {
      // This test would have caught the bug where frontend was applying 33% to already-reduced tokens
      // Backend sends: model_max=128000, max_input=42240 (already 33%)
      // Frontend should NOT multiply 42240 by 0.33 again

      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240, // Already 33% of 128000
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Should display "100 of 42,240 tokens" NOT "100 of 13,939 tokens"
      expect(container.textContent).toContain('100');
      expect(container.textContent).toContain('42,240');
      expect(container.textContent).not.toContain('13,939'); // Wrong value from double-multiply
    });

    it('should NOT double-apply strategy multiplier for "medium" strategy', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'medium',
        model_max_tokens: 128000,
        max_input_tokens: 84480, // Already 66% of 128000
        input_tokens_used: 200,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 50,
          user_messages: 75,
          assistant_messages: 75,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Should display "200 of 84,480 tokens"
      expect(container.textContent).toContain('200');
      expect(container.textContent).toContain('84,480');
    });

    it('should show full tokens for "full" strategy', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'full',
        model_max_tokens: 128000,
        max_input_tokens: 128000, // Full = 100%
        input_tokens_used: 500,
        messages_included: 10,
        messages_excluded: 0,
        breakdown: {
          system_messages: 100,
          user_messages: 200,
          assistant_messages: 200,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      expect(container.textContent).toContain('500');
      expect(container.textContent).toContain('128,000');
    });
  });

  describe('forbidden space calculation', () => {
    it('should calculate 67% forbidden space for "low" strategy', () => {
      // Low = 33% allowed, so 67% forbidden
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Forbidden tokens = 128000 - 42240 = 85760
      // Forbidden % = 85760 / 128000 â‰ˆ 67%
      // We can't easily test the visual representation, but we can verify the data
      expect(contextUtilization.model_max_tokens - contextUtilization.max_input_tokens).toBe(85760);
    });

    it('should calculate 34% forbidden space for "medium" strategy', () => {
      // Medium = 66% allowed, so 34% forbidden
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'medium',
        model_max_tokens: 128000,
        max_input_tokens: 84480,
        input_tokens_used: 200,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 50,
          user_messages: 75,
          assistant_messages: 75,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Forbidden tokens = 128000 - 84480 = 43520
      expect(contextUtilization.model_max_tokens - contextUtilization.max_input_tokens).toBe(43520);
    });

    it('should calculate 0% forbidden space for "full" strategy', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'full',
        model_max_tokens: 128000,
        max_input_tokens: 128000,
        input_tokens_used: 500,
        messages_included: 10,
        messages_excluded: 0,
        breakdown: {
          system_messages: 100,
          user_messages: 200,
          assistant_messages: 200,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // No forbidden space for full strategy
      expect(contextUtilization.model_max_tokens - contextUtilization.max_input_tokens).toBe(0);
    });
  });

  describe('number formatting', () => {
    it('should format large numbers with commas', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'full',
        model_max_tokens: 128000,
        max_input_tokens: 128000,
        input_tokens_used: 99999,
        messages_included: 50,
        messages_excluded: 10,
        breakdown: {
          system_messages: 1000,
          user_messages: 50000,
          assistant_messages: 48999,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Should have formatted numbers with commas
      expect(container.textContent).toContain('99,999');
      expect(container.textContent).toContain('128,000');
    });
  });

  describe('token breakdown percentages', () => {
    it('should calculate percentages relative to used tokens for tooltip', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 20, // 20%
          user_messages: 30,   // 30%
          assistant_messages: 50, // 50%
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Total = 100 tokens
      // System: 20/100 = 20%
      // User: 30/100 = 30%
      // Assistant: 50/100 = 50%
      expect(container.textContent).toContain('20.0%');
      expect(container.textContent).toContain('30.0%');
      expect(container.textContent).toContain('50.0%');
    });
  });

  describe('tooltip content', () => {
    it('should display both model_max_tokens and max_input_tokens', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 2,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Should show both values in tooltip
      expect(container.textContent).toContain('128,000'); // Model Max
      expect(container.textContent).toContain('42,240');  // Allowed Tokens
    });

    it('should display model name in tooltip', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      expect(container.textContent).toContain('gpt-4o-mini');
    });

    it('should display strategy in tooltip', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 0,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      // Strategy should be capitalized in tooltip
      expect(container.textContent).toMatch(/[Ll]ow/);
    });

    it('should show messages excluded count when > 0', () => {
      const contextUtilization: Usage['context_utilization'] = {
        model_name: 'gpt-4o-mini',
        strategy: 'low',
        model_max_tokens: 128000,
        max_input_tokens: 42240,
        input_tokens_used: 100,
        messages_included: 5,
        messages_excluded: 3,
        breakdown: {
          system_messages: 20,
          user_messages: 40,
          assistant_messages: 40,
        },
      };

      const { container } = renderWithProvider(
        <ContextUtilizationBadge contextUtilization={contextUtilization} />
      );

      expect(container.textContent).toContain('3'); // Excluded count
    });
  });
});

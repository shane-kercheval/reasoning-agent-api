{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM Metadata Exploration\n",
    "\n",
    "This notebook shows what metadata we get from litellm calls and what's available to save in the database.\n",
    "\n",
    "Focus: Understanding usage/cost data and other metadata from both streaming and non-streaming responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Simple test messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2? Be concise.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Streaming Response\n",
    "\n",
    "Shows all available metadata from a standard completion call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NON-STREAMING RESPONSE\n",
      "============================================================\n",
      "\n",
      "Response type: <class 'litellm.types.utils.ModelResponse'>\n",
      "\n",
      "Response object attributes:\n",
      "  - choices\n",
      "  - construct\n",
      "  - copy\n",
      "  - created\n",
      "  - dict\n",
      "  - from_orm\n",
      "  - get\n",
      "  - id\n",
      "  - json\n",
      "  - model\n",
      "  - model_computed_fields\n",
      "  - model_config\n",
      "  - model_construct\n",
      "  - model_copy\n",
      "  - model_dump\n",
      "  - model_dump_json\n",
      "  - model_extra\n",
      "  - model_fields\n",
      "  - model_fields_set\n",
      "  - model_json_schema\n",
      "  - model_parametrized_name\n",
      "  - model_post_init\n",
      "  - model_rebuild\n",
      "  - model_validate\n",
      "  - model_validate_json\n",
      "  - model_validate_strings\n",
      "  - object\n",
      "  - parse_file\n",
      "  - parse_obj\n",
      "  - parse_raw\n",
      "  - schema\n",
      "  - schema_json\n",
      "  - system_fingerprint\n",
      "  - to_dict\n",
      "  - to_json\n",
      "  - update_forward_refs\n",
      "  - validate\n",
      "\n",
      "============================================================\n",
      "KEY METADATA FIELDS\n",
      "============================================================\n",
      "\n",
      "ID: chatcmpl-CZ4B8pVMaxDZmVBx5PByTiuirXvZr\n",
      "Created: 1762473578\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "\n",
      "Usage object: Usage(completion_tokens=8, prompt_tokens=17, total_tokens=25, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n",
      "  Prompt tokens: 17\n",
      "  Completion tokens: 8\n",
      "  Total tokens: 25\n",
      "\n",
      "  Full usage dict: {\n",
      "  \"completion_tokens\": 8,\n",
      "  \"prompt_tokens\": 17,\n",
      "  \"total_tokens\": 25,\n",
      "  \"completion_tokens_details\": {\n",
      "    \"accepted_prediction_tokens\": 0,\n",
      "    \"audio_tokens\": 0,\n",
      "    \"reasoning_tokens\": 0,\n",
      "    \"rejected_prediction_tokens\": 0,\n",
      "    \"text_tokens\": null\n",
      "  },\n",
      "  \"prompt_tokens_details\": {\n",
      "    \"audio_tokens\": 0,\n",
      "    \"cached_tokens\": 0,\n",
      "    \"text_tokens\": null,\n",
      "    \"image_tokens\": null\n",
      "  }\n",
      "}\n",
      "\n",
      "Choices: 1\n",
      "  Finish reason: stop\n",
      "  Message role: assistant\n",
      "  Message content: 2 + 2 = 4.\n",
      "\n",
      "============================================================\n",
      "LITELLM INTERNAL METADATA\n",
      "============================================================\n",
      "Hidden params: {\n",
      "  \"custom_llm_provider\": \"openai\",\n",
      "  \"region_name\": null,\n",
      "  \"headers\": {\n",
      "    \"date\": \"Thu, 06 Nov 2025 23:59:39 GMT\",\n",
      "    \"content-type\": \"application/json\",\n",
      "    \"transfer-encoding\": \"chunked\",\n",
      "    \"connection\": \"keep-alive\",\n",
      "    \"access-control-expose-headers\": \"X-Request-ID\",\n",
      "    \"openai-organization\": \"user-usmsjuw4iekldvgzish9arbj\",\n",
      "    \"openai-processing-ms\": \"501\",\n",
      "    \"openai-project\": \"proj_KtM5UkidyPgbtwnmYeUEkdVa\",\n",
      "    \"openai-version\": \"2020-10-01\",\n",
      "    \"x-envoy-upstream-service-time\": \"705\",\n",
      "    \"x-ratelimit-limit-requests\": \"5000\",\n",
      "    \"x-ratelimit-limit-tokens\": \"4000000\",\n",
      "    \"x-ratelimit-remaining-requests\": \"4999\",\n",
      "    \"x-ratelimit-remaining-tokens\": \"3999992\",\n",
      "    \"x-ratelimit-reset-requests\": \"12ms\",\n",
      "    \"x-ratelimit-reset-tokens\": \"0s\",\n",
      "    \"x-request-id\": \"req_dca8e6d1df204eba80c809343d337058\",\n",
      "    \"x-openai-proxy-wasm\": \"v0.1\",\n",
      "    \"cf-cache-status\": \"DYNAMIC\",\n",
      "    \"set-cookie\": \"__cf_bm=SUNdpfinvXZootJXoeZCpGT5r3WuBcgyYLbLoYOC1Dw-1762473579-1.0.1.1-fE_iIcb5jRREFeh37HBDC7_L9tmAlNBJ3i74L6V_ZxPgwI437yElIGMGCwDljIl_tciCm_yyznzTvvk902r63ILCs0pnmf6MPMHwY97P9Fw; path=/; expires=Fri, 07-Nov-25 00:29:39 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=6pmDr66MYRYMDx4flWpLPNUoN2bvYdzc_VD15Il2vlI-1762473579373-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None\",\n",
      "    \"strict-transport-security\": \"max-age=31536000; includeSubDomains; preload\",\n",
      "    \"x-content-type-options\": \"nosniff\",\n",
      "    \"server\": \"cloudflare\",\n",
      "    \"cf-ray\": \"99a88bba2f13a0f3-SEA\",\n",
      "    \"content-encoding\": \"gzip\",\n",
      "    \"alt-svc\": \"h3=\\\":443\\\"; ma=86400\"\n",
      "  },\n",
      "  \"additional_headers\": {\n",
      "    \"x-ratelimit-limit-requests\": \"5000\",\n",
      "    \"x-ratelimit-remaining-requests\": \"4999\",\n",
      "    \"x-ratelimit-limit-tokens\": \"4000000\",\n",
      "    \"x-ratelimit-remaining-tokens\": \"3999992\",\n",
      "    \"llm_provider-date\": \"Thu, 06 Nov 2025 23:59:39 GMT\",\n",
      "    \"llm_provider-content-type\": \"application/json\",\n",
      "    \"llm_provider-transfer-encoding\": \"chunked\",\n",
      "    \"llm_provider-connection\": \"keep-alive\",\n",
      "    \"llm_provider-access-control-expose-headers\": \"X-Request-ID\",\n",
      "    \"llm_provider-openai-organization\": \"user-usmsjuw4iekldvgzish9arbj\",\n",
      "    \"llm_provider-openai-processing-ms\": \"501\",\n",
      "    \"llm_provider-openai-project\": \"proj_KtM5UkidyPgbtwnmYeUEkdVa\",\n",
      "    \"llm_provider-openai-version\": \"2020-10-01\",\n",
      "    \"llm_provider-x-envoy-upstream-service-time\": \"705\",\n",
      "    \"llm_provider-x-ratelimit-limit-requests\": \"5000\",\n",
      "    \"llm_provider-x-ratelimit-limit-tokens\": \"4000000\",\n",
      "    \"llm_provider-x-ratelimit-remaining-requests\": \"4999\",\n",
      "    \"llm_provider-x-ratelimit-remaining-tokens\": \"3999992\",\n",
      "    \"llm_provider-x-ratelimit-reset-requests\": \"12ms\",\n",
      "    \"llm_provider-x-ratelimit-reset-tokens\": \"0s\",\n",
      "    \"llm_provider-x-request-id\": \"req_dca8e6d1df204eba80c809343d337058\",\n",
      "    \"llm_provider-x-openai-proxy-wasm\": \"v0.1\",\n",
      "    \"llm_provider-cf-cache-status\": \"DYNAMIC\",\n",
      "    \"llm_provider-set-cookie\": \"__cf_bm=SUNdpfinvXZootJXoeZCpGT5r3WuBcgyYLbLoYOC1Dw-1762473579-1.0.1.1-fE_iIcb5jRREFeh37HBDC7_L9tmAlNBJ3i74L6V_ZxPgwI437yElIGMGCwDljIl_tciCm_yyznzTvvk902r63ILCs0pnmf6MPMHwY97P9Fw; path=/; expires=Fri, 07-Nov-25 00:29:39 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=6pmDr66MYRYMDx4flWpLPNUoN2bvYdzc_VD15Il2vlI-1762473579373-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None\",\n",
      "    \"llm_provider-strict-transport-security\": \"max-age=31536000; includeSubDomains; preload\",\n",
      "    \"llm_provider-x-content-type-options\": \"nosniff\",\n",
      "    \"llm_provider-server\": \"cloudflare\",\n",
      "    \"llm_provider-cf-ray\": \"99a88bba2f13a0f3-SEA\",\n",
      "    \"llm_provider-content-encoding\": \"gzip\",\n",
      "    \"llm_provider-alt-svc\": \"h3=\\\":443\\\"; ma=86400\"\n",
      "  },\n",
      "  \"optional_params\": {\n",
      "    \"temperature\": 0.0,\n",
      "    \"extra_body\": {}\n",
      "  },\n",
      "  \"litellm_call_id\": \"f4a2a06c-4199-4d5c-a1d4-a638e4109054\",\n",
      "  \"api_base\": \"https://api.openai.com\",\n",
      "  \"model_id\": null,\n",
      "  \"response_cost\": 7.349999999999999e-06,\n",
      "  \"litellm_model_name\": \"gpt-4o-mini\",\n",
      "  \"_response_ms\": 1331.7730000000001,\n",
      "  \"litellm_overhead_time_ms\": 18.962\n",
      "}\n",
      "\n",
      "============================================================\n",
      "FULL RESPONSE DICT\n",
      "============================================================\n",
      "{\n",
      "  \"id\": \"chatcmpl-CZ4B8pVMaxDZmVBx5PByTiuirXvZr\",\n",
      "  \"created\": 1762473578,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_560af6e559\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"2 + 2 = 4.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": null,\n",
      "        \"function_call\": null,\n",
      "        \"annotations\": []\n",
      "      },\n",
      "      \"provider_specific_fields\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 8,\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"total_tokens\": 25,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0,\n",
      "      \"text_tokens\": null\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0,\n",
      "      \"text_tokens\": null,\n",
      "      \"image_tokens\": null\n",
      "    }\n",
      "  },\n",
      "  \"service_tier\": \"default\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Non-streaming call (similar to reasoning_agent.py step generation)\n",
    "response = await litellm.acompletion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NON-STREAMING RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResponse type: {type(response)}\")\n",
    "print(\"\\nResponse object attributes:\")\n",
    "for attr in dir(response):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"  - {attr}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY METADATA FIELDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ID and timestamps\n",
    "print(f\"\\nID: {response.id}\")\n",
    "print(f\"Created: {response.created}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "\n",
    "# Usage information\n",
    "print(f\"\\nUsage object: {response.usage}\")\n",
    "if response.usage:\n",
    "    print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "    print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
    "    print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "    # Cost information (if available from litellm)\n",
    "    usage_dict = response.usage.model_dump() if hasattr(response.usage, 'model_dump') else \\\n",
    "        response.usage.__dict__\n",
    "    print(f\"\\n  Full usage dict: {json.dumps(usage_dict, indent=2, default=str)}\")\n",
    "\n",
    "# Response content\n",
    "print(f\"\\nChoices: {len(response.choices)}\")\n",
    "if response.choices:\n",
    "    choice = response.choices[0]\n",
    "    print(f\"  Finish reason: {choice.finish_reason}\")\n",
    "    print(f\"  Message role: {choice.message.role}\")\n",
    "    print(f\"  Message content: {choice.message.content}\")\n",
    "\n",
    "# Hidden metadata (litellm-specific)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LITELLM INTERNAL METADATA\")\n",
    "print(\"=\" * 60)\n",
    "if hasattr(response, '_hidden_params'):\n",
    "    print(f\"Hidden params: {json.dumps(response._hidden_params, indent=2, default=str)}\")\n",
    "\n",
    "# Full response as dict\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FULL RESPONSE DICT\")\n",
    "print(\"=\" * 60)\n",
    "response_dict = response.model_dump() if hasattr(response, 'model_dump') else response.__dict__\n",
    "print(json.dumps(response_dict, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Response\n",
    "\n",
    "Shows what metadata is available in streaming chunks (used in passthrough.py and final synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STREAMING RESPONSE CHUNKS\n",
      "============================================================\n",
      "\n",
      "Chunk 1:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: '2'\n",
      "  Role: assistant\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 2:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: ' +'\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 3:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: ' '\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 4:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: '2'\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 5:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: ' ='\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 6:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: ' '\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 7:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: '4'\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 8:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Content: '.'\n",
      "  Finish reason: None\n",
      "\n",
      "Chunk 9:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  Finish reason: stop\n",
      "\n",
      "Chunk 10:\n",
      "  Type: <class 'litellm.types.utils.ModelResponseStream'>\n",
      "  ID: chatcmpl-CZ4BAGcJBmqHwRDceEe69SGLy26ju\n",
      "  Model: gpt-4o-mini\n",
      "  Created: 1762473580\n",
      "  ✅ USAGE DATA FOUND:\n",
      "     Prompt tokens: 17\n",
      "     Completion tokens: 8\n",
      "     Total tokens: 25\n",
      "     Full usage dict: {\n",
      "  \"completion_tokens\": 8,\n",
      "  \"prompt_tokens\": 17,\n",
      "  \"total_tokens\": 25,\n",
      "  \"completion_tokens_details\": {\n",
      "    \"accepted_prediction_tokens\": 0,\n",
      "    \"audio_tokens\": 0,\n",
      "    \"reasoning_tokens\": 0,\n",
      "    \"rejected_prediction_tokens\": 0,\n",
      "    \"text_tokens\": null\n",
      "  },\n",
      "  \"prompt_tokens_details\": {\n",
      "    \"audio_tokens\": 0,\n",
      "    \"cached_tokens\": 0,\n",
      "    \"text_tokens\": null,\n",
      "    \"image_tokens\": null\n",
      "  }\n",
      "}\n",
      "  Finish reason: None\n",
      "\n",
      "============================================================\n",
      "STREAMING SUMMARY\n",
      "============================================================\n",
      "Total chunks: 10\n",
      "Usage data received: ✅ Yes\n",
      "  Total tokens: 25\n"
     ]
    }
   ],
   "source": [
    "# Streaming call (similar to passthrough.py)\n",
    "stream = await litellm.acompletion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True},  # Request usage in final chunk\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STREAMING RESPONSE CHUNKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "chunk_count = 0\n",
    "accumulated_usage = None\n",
    "\n",
    "async for chunk in stream:\n",
    "    chunk_count += 1\n",
    "    print(f\"\\nChunk {chunk_count}:\")\n",
    "    print(f\"  Type: {type(chunk)}\")\n",
    "    print(f\"  ID: {chunk.id}\")\n",
    "    print(f\"  Model: {chunk.model}\")\n",
    "    print(f\"  Created: {chunk.created}\")\n",
    "\n",
    "    # Check for usage (only in final chunk with stream_options)\n",
    "    chunk_usage = getattr(chunk, 'usage', None)\n",
    "    if chunk_usage:\n",
    "        print(\"  ✅ USAGE DATA FOUND:\")\n",
    "        print(f\"     Prompt tokens: {chunk_usage.prompt_tokens}\")\n",
    "        print(f\"     Completion tokens: {chunk_usage.completion_tokens}\")\n",
    "        print(f\"     Total tokens: {chunk_usage.total_tokens}\")\n",
    "        accumulated_usage = chunk_usage\n",
    "\n",
    "        # Check for cost data\n",
    "        usage_dict = chunk_usage.model_dump() if hasattr(chunk_usage, 'model_dump') \\\n",
    "            else chunk_usage.__dict__\n",
    "        print(f\"     Full usage dict: {json.dumps(usage_dict, indent=2, default=str)}\")\n",
    "\n",
    "    # Delta content\n",
    "    if chunk.choices:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if hasattr(delta, 'content') and delta.content:\n",
    "            print(f\"  Content: {delta.content!r}\")\n",
    "        if hasattr(delta, 'role') and delta.role:\n",
    "            print(f\"  Role: {delta.role}\")\n",
    "        print(f\"  Finish reason: {chunk.choices[0].finish_reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STREAMING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total chunks: {chunk_count}\")\n",
    "print(f\"Usage data received: {'✅ Yes' if accumulated_usage else '❌ No'}\")\n",
    "if accumulated_usage:\n",
    "    print(f\"  Total tokens: {accumulated_usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Calculation\n",
    "\n",
    "LiteLLM can track costs using `litellm.completion_cost()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COST CALCULATION\n",
      "============================================================\n",
      "\n",
      "Total cost: $0.000007\n",
      "Prompt cost: $0.000005\n",
      "Completion cost: $0.000002\n"
     ]
    }
   ],
   "source": [
    "# Cost calculation using litellm helper\n",
    "response = await litellm.acompletion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COST CALCULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate cost using litellm\n",
    "try:\n",
    "    cost = litellm.completion_cost(completion_response=response)\n",
    "    print(f\"\\nTotal cost: ${cost:.6f}\")\n",
    "\n",
    "    # Calculate per-token costs\n",
    "    if response.usage:\n",
    "        prompt_cost = (cost / response.usage.total_tokens) * response.usage.prompt_tokens\n",
    "        completion_cost = (cost / response.usage.total_tokens) * response.usage.completion_tokens\n",
    "        print(f\"Prompt cost: ${prompt_cost:.6f}\")\n",
    "        print(f\"Completion cost: ${completion_cost:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cost calculation error: {e}\")\n",
    "\n",
    "# Alternative: Access cost from response if litellm adds it\n",
    "response_dict = response.model_dump() if hasattr(response, 'model_dump') else response.__dict__\n",
    "if '_hidden_params' in response_dict:\n",
    "    hidden = response_dict['_hidden_params']\n",
    "    if 'response_cost' in hidden:\n",
    "        print(f\"\\nCost from _hidden_params: ${hidden['response_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Metadata to Store\n",
    "\n",
    "Based on what's available from litellm, here's what we should consider storing in the `messages.metadata` JSONB field:\n",
    "\n",
    "```python\n",
    "{\n",
    "    # Token usage\n",
    "    \"prompt_tokens\": 10,\n",
    "    \"completion_tokens\": 5,\n",
    "    \"total_tokens\": 15,\n",
    "    \n",
    "    # Costs (calculated via litellm.completion_cost)\n",
    "    \"prompt_cost\": 0.000015,\n",
    "    \"completion_cost\": 0.000075,\n",
    "    \"total_cost\": 0.000090,\n",
    "    \n",
    "    # Model info\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"completion_id\": \"chatcmpl-...\",\n",
    "    \n",
    "    # Request parameters\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_tokens\": 1000,\n",
    "    \n",
    "    # Timing\n",
    "    \"created_at\": 1699564800,\n",
    "    \"finish_reason\": \"stop\",\n",
    "    \n",
    "    # Routing (for debugging)\n",
    "    \"routing_path\": \"passthrough\",  # or \"reasoning\", \"orchestration\"\n",
    "    \"session_id\": \"...\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"user\": \"role\", \"content\": \"Hey, how's it going\"}]\n",
    "print(token_counter(model=\"gpt-4o-mini\", messages=messages))\n",
    "print(token_counter(model=\"claude-haiku-4-5\", messages=messages))\n",
    "print(token_counter(model=\"claude-haiku-4-5\", text=\"Hey, how's it going\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n",
      "200000\n",
      "{\n",
      "  \"key\": \"gpt-4o-mini\",\n",
      "  \"max_tokens\": 16384,\n",
      "  \"max_input_tokens\": 128000,\n",
      "  \"max_output_tokens\": 16384,\n",
      "  \"input_cost_per_token\": 1.5e-07,\n",
      "  \"input_cost_per_token_flex\": null,\n",
      "  \"input_cost_per_token_priority\": 2.5e-07,\n",
      "  \"cache_creation_input_token_cost\": null,\n",
      "  \"cache_creation_input_token_cost_above_200k_tokens\": null,\n",
      "  \"cache_read_input_token_cost\": 7.5e-08,\n",
      "  \"cache_read_input_token_cost_above_200k_tokens\": null,\n",
      "  \"cache_read_input_token_cost_flex\": null,\n",
      "  \"cache_read_input_token_cost_priority\": 1.25e-07,\n",
      "  \"cache_creation_input_token_cost_above_1hr\": null,\n",
      "  \"input_cost_per_character\": null,\n",
      "  \"input_cost_per_token_above_128k_tokens\": null,\n",
      "  \"input_cost_per_token_above_200k_tokens\": null,\n",
      "  \"input_cost_per_query\": null,\n",
      "  \"input_cost_per_second\": null,\n",
      "  \"input_cost_per_audio_token\": null,\n",
      "  \"input_cost_per_token_batches\": 7.5e-08,\n",
      "  \"output_cost_per_token_batches\": 3e-07,\n",
      "  \"output_cost_per_token\": 6e-07,\n",
      "  \"output_cost_per_token_flex\": null,\n",
      "  \"output_cost_per_token_priority\": 1e-06,\n",
      "  \"output_cost_per_audio_token\": null,\n",
      "  \"output_cost_per_character\": null,\n",
      "  \"output_cost_per_reasoning_token\": null,\n",
      "  \"output_cost_per_token_above_128k_tokens\": null,\n",
      "  \"output_cost_per_character_above_128k_tokens\": null,\n",
      "  \"output_cost_per_token_above_200k_tokens\": null,\n",
      "  \"output_cost_per_second\": null,\n",
      "  \"output_cost_per_video_per_second\": null,\n",
      "  \"output_cost_per_image\": null,\n",
      "  \"output_vector_size\": null,\n",
      "  \"citation_cost_per_token\": null,\n",
      "  \"tiered_pricing\": null,\n",
      "  \"litellm_provider\": \"openai\",\n",
      "  \"mode\": \"chat\",\n",
      "  \"supports_system_messages\": true,\n",
      "  \"supports_response_schema\": true,\n",
      "  \"supports_vision\": true,\n",
      "  \"supports_function_calling\": true,\n",
      "  \"supports_tool_choice\": true,\n",
      "  \"supports_assistant_prefill\": null,\n",
      "  \"supports_prompt_caching\": true,\n",
      "  \"supports_audio_input\": null,\n",
      "  \"supports_audio_output\": null,\n",
      "  \"supports_pdf_input\": true,\n",
      "  \"supports_embedding_image_input\": null,\n",
      "  \"supports_native_streaming\": null,\n",
      "  \"supports_web_search\": null,\n",
      "  \"supports_url_context\": null,\n",
      "  \"supports_reasoning\": null,\n",
      "  \"supports_computer_use\": null,\n",
      "  \"search_context_cost_per_query\": null,\n",
      "  \"tpm\": null,\n",
      "  \"rpm\": null,\n",
      "  \"ocr_cost_per_page\": null,\n",
      "  \"annotation_cost_per_page\": null,\n",
      "  \"supported_openai_params\": [\n",
      "    \"frequency_penalty\",\n",
      "    \"logit_bias\",\n",
      "    \"logprobs\",\n",
      "    \"top_logprobs\",\n",
      "    \"max_tokens\",\n",
      "    \"max_completion_tokens\",\n",
      "    \"modalities\",\n",
      "    \"prediction\",\n",
      "    \"n\",\n",
      "    \"presence_penalty\",\n",
      "    \"seed\",\n",
      "    \"stop\",\n",
      "    \"stream\",\n",
      "    \"stream_options\",\n",
      "    \"temperature\",\n",
      "    \"top_p\",\n",
      "    \"tools\",\n",
      "    \"tool_choice\",\n",
      "    \"function_call\",\n",
      "    \"functions\",\n",
      "    \"max_retries\",\n",
      "    \"extra_headers\",\n",
      "    \"parallel_tool_calls\",\n",
      "    \"audio\",\n",
      "    \"web_search_options\",\n",
      "    \"service_tier\",\n",
      "    \"safety_identifier\",\n",
      "    \"response_format\",\n",
      "    \"user\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import json\n",
    "# Get max input tokens for a specific model\n",
    "print(litellm.get_model_info(\"gpt-4o-mini\")['max_input_tokens'])\n",
    "print(litellm.get_model_info(\"claude-haiku-4-5\")['max_input_tokens'])\n",
    "print(json.dumps(litellm.get_model_info(\"gpt-4o-mini\"), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
